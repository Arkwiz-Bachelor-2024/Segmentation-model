We are running from this directory: /cluster/work/petteed/seg_model/Segmentation-model
---------------------------------------------------------
The name of the job is: Deeplabv3plus_100e_16b_Poly_Adam_lower_wBCE_milestones_warmup
The job ID is 19633639
---------------------------------------------------------
Number of GPUs : 4,5
---------------------------------------------------------
Assert Enviroment modules are loaded...
---------------------------------------------------------
Assert python modules are loaded....
Requirement already satisfied: scikit-learn in /cluster/home/petteed/.local/lib/python3.10/site-packages (1.4.1.post1)
Requirement already satisfied: numpy<2.0,>=1.19.5 in /cluster/apps/eb/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages (from scikit-learn) (1.22.3)
Requirement already satisfied: joblib>=1.2.0 in /cluster/home/petteed/.local/lib/python3.10/site-packages (from scikit-learn) (1.3.2)
Requirement already satisfied: threadpoolctl>=2.0.0 in /cluster/apps/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from scikit-learn) (3.1.0)
Requirement already satisfied: scipy>=1.6.0 in /cluster/apps/eb/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages (from scikit-learn) (1.8.1)
Requirement already satisfied: matplotlib in /cluster/home/petteed/.local/lib/python3.10/site-packages (3.8.3)
Requirement already satisfied: packaging>=20.0 in /cluster/apps/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from matplotlib) (20.9)
Requirement already satisfied: fonttools>=4.22.0 in /cluster/home/petteed/.local/lib/python3.10/site-packages (from matplotlib) (4.50.0)
Requirement already satisfied: python-dateutil>=2.7 in /cluster/apps/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from matplotlib) (2.8.2)
Requirement already satisfied: cycler>=0.10 in /cluster/home/petteed/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)
Requirement already satisfied: contourpy>=1.0.1 in /cluster/home/petteed/.local/lib/python3.10/site-packages (from matplotlib) (1.2.0)
Requirement already satisfied: kiwisolver>=1.3.1 in /cluster/home/petteed/.local/lib/python3.10/site-packages (from matplotlib) (1.4.5)
Requirement already satisfied: numpy<2,>=1.21 in /cluster/apps/eb/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages (from matplotlib) (1.22.3)
Requirement already satisfied: pyparsing>=2.3.1 in /cluster/apps/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from matplotlib) (3.0.8)
Requirement already satisfied: pillow>=8 in /cluster/home/petteed/.local/lib/python3.10/site-packages (from matplotlib) (10.2.0)
Requirement already satisfied: six>=1.5 in /cluster/apps/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)
Requirement already satisfied: cython in /cluster/apps/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (0.29.28)
Collecting git+https://github.com/lucasb-eyer/pydensecrf.git
  Cloning https://github.com/lucasb-eyer/pydensecrf.git to /tmp/pip-req-build-xeb6i7tv
  Resolved https://github.com/lucasb-eyer/pydensecrf.git to commit 2723c7fa4f2ead16ae1ce3d8afe977724bb8f87f
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
---------------------------------------------------------
GPU specifications:
Mon Apr 15 23:23:24 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla V100-PCIE-32GB           On  | 00000000:89:00.0 Off |                    0 |
| N/A   28C    P0              22W / 250W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           On  | 00000000:8A:00.0 Off |                    0 |
| N/A   29C    P0              22W / 250W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
Training model:
---------------------------------------------------------------------------------------------------
Enviroment:
---------------------------------------------------------------------------------------------------
TensorFlow version: 2.11.0
GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]
---------------------------------------------------------------------------------------------------
Number of distributed devices: 2
---------------------------------------------------------------------------------------------------
Data shape
Image shape: (16, 512, 512, 3)
Mask shape: (16, 512, 512, 5)
Training dataset:  <BatchDataset element_spec=(TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 512, 512, 5), dtype=tf.float32, name=None))>
Validation dataset: <BatchDataset element_spec=(TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 512, 512, 5), dtype=tf.float32, name=None))>
---------------------------------------------------------------------------------------------------
Details:
---------------------------------------------------------------------------------------------------
Classes:  5
Global batch size: 16
Epochs:  100
Optimizer:  <class 'keras.optimizers.optimizer_experimental.adam.Adam'>
Initial learning rate:  0.0001
Base learning rate:  0.01
Warmup-batches:  20
Milestones:  [5, 15, 25, 40]
BCE weights:  [2.12, 35.52, 3.31, 12.87, 28.13]
---------------------------------------------------------------------------------------------------
Epoch 1/100

Epoch 1: val_loss improved from inf to 11.70111, saving model to ./models/Deeplabv3plus_100e_16b_Poly_Adam_lower_wBCE_milestones_warmup
467/467 - 774s - loss: 11.6243 - accuracy: 0.6043 - val_loss: 11.7011 - val_accuracy: 0.6002 - 774s/epoch - 2s/step
Epoch 2/100

Epoch 2: val_loss improved from 11.70111 to 11.54693, saving model to ./models/Deeplabv3plus_100e_16b_Poly_Adam_lower_wBCE_milestones_warmup

Epoch 1: Learning rate was 0.00990995.
467/467 - 463s - loss: 10.5761 - accuracy: 0.5745 - val_loss: 11.5469 - val_accuracy: 0.3821 - 463s/epoch - 992ms/step
Epoch 3/100

Epoch 3: val_loss did not improve from 11.54693

Epoch 2: Learning rate was 0.00981982.
467/467 - 450s - loss: 10.1545 - accuracy: 0.6015 - val_loss: 12.8377 - val_accuracy: 0.3457 - 450s/epoch - 965ms/step
Epoch 4/100

Epoch 4: val_loss did not improve from 11.54693

Epoch 3: Learning rate was 0.00972959.
467/467 - 445s - loss: 9.9520 - accuracy: 0.6270 - val_loss: 19.3539 - val_accuracy: 0.6182 - 445s/epoch - 954ms/step
Epoch 5/100

Epoch 5: val_loss did not improve from 11.54693

Epoch 4: Learning rate was 0.00963927.
467/467 - 435s - loss: 9.8369 - accuracy: 0.6297 - val_loss: 18.3901 - val_accuracy: 0.3401 - 435s/epoch - 931ms/step
Epoch 6/100

Epoch 6: val_loss improved from 11.54693 to 10.12959, saving model to ./models/Deeplabv3plus_100e_16b_Poly_Adam_lower_wBCE_milestones_warmup

Epoch 5: Learning rate was 0.000954885.
467/467 - 488s - loss: 9.7590 - accuracy: 0.6270 - val_loss: 10.1296 - val_accuracy: 0.6000 - 488s/epoch - 1s/step
Epoch 7/100

Epoch 7: val_loss improved from 10.12959 to 9.92788, saving model to ./models/Deeplabv3plus_100e_16b_Poly_Adam_lower_wBCE_milestones_warmup

Epoch 6: Learning rate was 0.000945834.
467/467 - 466s - loss: 9.4183 - accuracy: 0.6474 - val_loss: 9.9279 - val_accuracy: 0.6203 - 466s/epoch - 997ms/step
Epoch 8/100

Epoch 8: val_loss improved from 9.92788 to 8.81256, saving model to ./models/Deeplabv3plus_100e_16b_Poly_Adam_lower_wBCE_milestones_warmup

Epoch 7: Learning rate was 0.000936774.
467/467 - 474s - loss: 9.1833 - accuracy: 0.6624 - val_loss: 8.8126 - val_accuracy: 0.7384 - 474s/epoch - 1s/step
Epoch 9/100

Epoch 9: val_loss improved from 8.81256 to 8.36026, saving model to ./models/Deeplabv3plus_100e_16b_Poly_Adam_lower_wBCE_milestones_warmup

Epoch 8: Learning rate was 0.000927703.
467/467 - 467s - loss: 9.0735 - accuracy: 0.6693 - val_loss: 8.3603 - val_accuracy: 0.7462 - 467s/epoch - 999ms/step
Epoch 10/100

Epoch 10: val_loss did not improve from 8.36026

Epoch 9: Learning rate was 0.000918623.
467/467 - 447s - loss: 8.9961 - accuracy: 0.6726 - val_loss: 8.3908 - val_accuracy: 0.7355 - 447s/epoch - 957ms/step
Epoch 11/100

Epoch 11: val_loss did not improve from 8.36026

Epoch 10: Learning rate was 0.000909533.
467/467 - 435s - loss: 8.8994 - accuracy: 0.6786 - val_loss: 8.5132 - val_accuracy: 0.7256 - 435s/epoch - 931ms/step
Epoch 12/100

Epoch 12: val_loss did not improve from 8.36026

Epoch 11: Learning rate was 0.000900432.
467/467 - 426s - loss: 8.8296 - accuracy: 0.6864 - val_loss: 8.4314 - val_accuracy: 0.7324 - 426s/epoch - 913ms/step
Epoch 13/100

Epoch 13: val_loss did not improve from 8.36026

Epoch 12: Learning rate was 0.000891322.
467/467 - 432s - loss: 8.7567 - accuracy: 0.6896 - val_loss: 8.3929 - val_accuracy: 0.7364 - 432s/epoch - 924ms/step
Epoch 14/100

Epoch 14: val_loss improved from 8.36026 to 8.21177, saving model to ./models/Deeplabv3plus_100e_16b_Poly_Adam_lower_wBCE_milestones_warmup

Epoch 13: Learning rate was 0.000882201.
467/467 - 479s - loss: 8.6681 - accuracy: 0.6937 - val_loss: 8.2118 - val_accuracy: 0.7226 - 479s/epoch - 1s/step
Epoch 15/100

Epoch 15: val_loss improved from 8.21177 to 8.14890, saving model to ./models/Deeplabv3plus_100e_16b_Poly_Adam_lower_wBCE_milestones_warmup

Epoch 14: Learning rate was 0.000873069.
467/467 - 482s - loss: 8.6231 - accuracy: 0.6965 - val_loss: 8.1489 - val_accuracy: 0.7296 - 482s/epoch - 1s/step
Epoch 16/100

Epoch 16: val_loss did not improve from 8.14890

Epoch 15: Learning rate was 8.63927e-05.
467/467 - 432s - loss: 8.5225 - accuracy: 0.7007 - val_loss: 8.9276 - val_accuracy: 0.7148 - 432s/epoch - 925ms/step
Epoch 17/100

Epoch 17: val_loss improved from 8.14890 to 8.00001, saving model to ./models/Deeplabv3plus_100e_16b_Poly_Adam_lower_wBCE_milestones_warmup

Epoch 16: Learning rate was 8.54774e-05.
467/467 - 497s - loss: 8.4727 - accuracy: 0.6997 - val_loss: 8.0000 - val_accuracy: 0.7534 - 497s/epoch - 1s/step
Epoch 18/100

Epoch 18: val_loss improved from 8.00001 to 7.97177, saving model to ./models/Deeplabv3plus_100e_16b_Poly_Adam_lower_wBCE_milestones_warmup

Epoch 17: Learning rate was 8.4561e-05.
467/467 - 488s - loss: 8.4013 - accuracy: 0.7005 - val_loss: 7.9718 - val_accuracy: 0.7513 - 488s/epoch - 1s/step
Epoch 19/100

Epoch 19: val_loss did not improve from 7.97177

Epoch 18: Learning rate was 8.36436e-05.
467/467 - 443s - loss: 8.3712 - accuracy: 0.7013 - val_loss: 8.0078 - val_accuracy: 0.7555 - 443s/epoch - 949ms/step
Epoch 20/100

Epoch 20: val_loss improved from 7.97177 to 7.93643, saving model to ./models/Deeplabv3plus_100e_16b_Poly_Adam_lower_wBCE_milestones_warmup

Epoch 19: Learning rate was 8.2725e-05.
467/467 - 497s - loss: 8.3561 - accuracy: 0.7027 - val_loss: 7.9364 - val_accuracy: 0.7556 - 497s/epoch - 1s/step
Epoch 21/100

Epoch 21: val_loss improved from 7.93643 to 7.92148, saving model to ./models/Deeplabv3plus_100e_16b_Poly_Adam_lower_wBCE_milestones_warmup

Epoch 20: Learning rate was 8.18052e-05.
467/467 - 478s - loss: 8.3319 - accuracy: 0.7031 - val_loss: 7.9215 - val_accuracy: 0.7551 - 478s/epoch - 1s/step
Epoch 22/100

Epoch 22: val_loss did not improve from 7.92148

Epoch 21: Learning rate was 8.08843e-05.
467/467 - 455s - loss: 8.3065 - accuracy: 0.7080 - val_loss: 7.9717 - val_accuracy: 0.7551 - 455s/epoch - 973ms/step
Epoch 23/100

Epoch 23: val_loss did not improve from 7.92148

Epoch 22: Learning rate was 7.99623e-05.
467/467 - 444s - loss: 8.3017 - accuracy: 0.7061 - val_loss: 7.9408 - val_accuracy: 0.7505 - 444s/epoch - 951ms/step
Epoch 24/100

Epoch 24: val_loss improved from 7.92148 to 7.89652, saving model to ./models/Deeplabv3plus_100e_16b_Poly_Adam_lower_wBCE_milestones_warmup

Epoch 23: Learning rate was 7.9039e-05.
467/467 - 494s - loss: 8.3301 - accuracy: 0.7023 - val_loss: 7.8965 - val_accuracy: 0.7542 - 494s/epoch - 1s/step
Epoch 25/100

Epoch 25: val_loss did not improve from 7.89652

Epoch 24: Learning rate was 7.81146e-05.
467/467 - 452s - loss: 8.2975 - accuracy: 0.7044 - val_loss: 7.9520 - val_accuracy: 0.7561 - 452s/epoch - 968ms/step
Epoch 26/100

Epoch 26: val_loss did not improve from 7.89652

Epoch 25: Learning rate was 7.7189e-06.
467/467 - 443s - loss: 8.2453 - accuracy: 0.7074 - val_loss: 7.9305 - val_accuracy: 0.7560 - 443s/epoch - 948ms/step
Epoch 27/100

Epoch 27: val_loss did not improve from 7.89652

Epoch 26: Learning rate was 7.62621e-06.
467/467 - 451s - loss: 8.2291 - accuracy: 0.7075 - val_loss: 7.9210 - val_accuracy: 0.7573 - 451s/epoch - 967ms/step
Epoch 28/100

Epoch 28: val_loss improved from 7.89652 to 7.83836, saving model to ./models/Deeplabv3plus_100e_16b_Poly_Adam_lower_wBCE_milestones_warmup

Epoch 27: Learning rate was 7.53339e-06.
467/467 - 466s - loss: 8.2574 - accuracy: 0.7050 - val_loss: 7.8384 - val_accuracy: 0.7581 - 466s/epoch - 998ms/step
Epoch 29/100

Epoch 29: val_loss did not improve from 7.83836

Epoch 28: Learning rate was 7.44045e-06.
467/467 - 442s - loss: 8.2357 - accuracy: 0.7073 - val_loss: 7.8899 - val_accuracy: 0.7567 - 442s/epoch - 946ms/step
Epoch 30/100

Epoch 30: val_loss did not improve from 7.83836

Epoch 29: Learning rate was 7.34738e-06.
467/467 - 451s - loss: 8.2134 - accuracy: 0.7101 - val_loss: 7.9385 - val_accuracy: 0.7544 - 451s/epoch - 966ms/step
Epoch 31/100

Epoch 31: val_loss did not improve from 7.83836

Epoch 30: Learning rate was 7.25418e-06.
467/467 - 438s - loss: 8.2174 - accuracy: 0.7094 - val_loss: 7.8904 - val_accuracy: 0.7573 - 438s/epoch - 938ms/step
Epoch 32/100

Epoch 32: val_loss improved from 7.83836 to 7.77250, saving model to ./models/Deeplabv3plus_100e_16b_Poly_Adam_lower_wBCE_milestones_warmup

Epoch 31: Learning rate was 7.16084e-06.
467/467 - 474s - loss: 8.2124 - accuracy: 0.7112 - val_loss: 7.7725 - val_accuracy: 0.7569 - 474s/epoch - 1s/step
Epoch 33/100

Epoch 33: val_loss did not improve from 7.77250

Epoch 32: Learning rate was 7.06737e-06.
467/467 - 433s - loss: 8.2220 - accuracy: 0.7089 - val_loss: 7.8048 - val_accuracy: 0.7577 - 433s/epoch - 928ms/step
Epoch 34/100

Epoch 34: val_loss did not improve from 7.77250

Epoch 33: Learning rate was 6.97377e-06.
467/467 - 447s - loss: 8.2449 - accuracy: 0.7064 - val_loss: 7.9005 - val_accuracy: 0.7583 - 447s/epoch - 958ms/step
Epoch 35/100

Epoch 35: val_loss did not improve from 7.77250

Epoch 34: Learning rate was 6.88002e-06.
467/467 - 425s - loss: 8.2120 - accuracy: 0.7104 - val_loss: 7.9710 - val_accuracy: 0.7555 - 425s/epoch - 910ms/step
Epoch 36/100

Epoch 36: val_loss did not improve from 7.77250

Epoch 35: Learning rate was 6.78613e-06.
467/467 - 443s - loss: 8.2345 - accuracy: 0.7086 - val_loss: 7.9240 - val_accuracy: 0.7553 - 443s/epoch - 949ms/step
Epoch 37/100

Epoch 37: val_loss did not improve from 7.77250

Epoch 36: Learning rate was 6.69209e-06.
467/467 - 447s - loss: 8.2185 - accuracy: 0.7092 - val_loss: 7.9370 - val_accuracy: 0.7557 - 447s/epoch - 957ms/step
Epoch 38/100

Epoch 38: val_loss did not improve from 7.77250

Epoch 37: Learning rate was 6.59791e-06.
467/467 - 426s - loss: 8.2221 - accuracy: 0.7085 - val_loss: 7.8564 - val_accuracy: 0.7539 - 426s/epoch - 913ms/step
Epoch 39/100

Epoch 39: val_loss did not improve from 7.77250

Epoch 38: Learning rate was 6.50358e-06.
467/467 - 449s - loss: 8.2107 - accuracy: 0.7117 - val_loss: 7.9023 - val_accuracy: 0.7575 - 449s/epoch - 961ms/step
Epoch 40/100

Epoch 40: val_loss did not improve from 7.77250

Epoch 39: Learning rate was 6.4091e-06.
467/467 - 421s - loss: 8.2276 - accuracy: 0.7084 - val_loss: 7.8656 - val_accuracy: 0.7554 - 421s/epoch - 902ms/step
Epoch 41/100

Epoch 41: val_loss did not improve from 7.77250

Epoch 40: Learning rate was 6.31446e-07.
467/467 - 429s - loss: 8.2130 - accuracy: 0.7100 - val_loss: 7.8491 - val_accuracy: 0.7524 - 429s/epoch - 919ms/step
Epoch 42/100

Epoch 42: val_loss did not improve from 7.77250

Epoch 41: Learning rate was 6.21966e-07.
467/467 - 434s - loss: 8.2050 - accuracy: 0.7115 - val_loss: 7.8493 - val_accuracy: 0.7545 - 434s/epoch - 930ms/step
Epoch 43/100

Epoch 43: val_loss did not improve from 7.77250

Epoch 42: Learning rate was 6.12471e-07.
467/467 - 449s - loss: 8.2079 - accuracy: 0.7093 - val_loss: 7.9065 - val_accuracy: 0.7573 - 449s/epoch - 961ms/step
Epoch 44/100

Epoch 44: val_loss did not improve from 7.77250

Epoch 43: Learning rate was 6.02958e-07.
467/467 - 434s - loss: 8.2046 - accuracy: 0.7105 - val_loss: 7.9121 - val_accuracy: 0.7551 - 434s/epoch - 930ms/step
Epoch 45/100

Epoch 45: val_loss did not improve from 7.77250

Epoch 44: Learning rate was 5.9343e-07.
467/467 - 431s - loss: 8.1840 - accuracy: 0.7112 - val_loss: 7.8726 - val_accuracy: 0.7572 - 431s/epoch - 923ms/step
Epoch 46/100

Epoch 46: val_loss did not improve from 7.77250

Epoch 45: Learning rate was 5.83884e-07.
467/467 - 427s - loss: 8.2220 - accuracy: 0.7090 - val_loss: 7.8710 - val_accuracy: 0.7562 - 427s/epoch - 915ms/step
Epoch 47/100

Epoch 47: val_loss did not improve from 7.77250

Epoch 46: Learning rate was 5.74321e-07.
467/467 - 434s - loss: 8.2072 - accuracy: 0.7105 - val_loss: 7.8946 - val_accuracy: 0.7543 - 434s/epoch - 930ms/step
Epoch 47: early stopping
---------------------------------------------------------
Script completed
