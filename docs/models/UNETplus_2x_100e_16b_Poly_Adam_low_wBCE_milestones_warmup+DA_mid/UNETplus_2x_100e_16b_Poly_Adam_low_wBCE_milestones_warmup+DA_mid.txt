We are running from this directory: /cluster/work/petteed/seg_model/Segmentation-model
---------------------------------------------------------
The name of the job is: UNETplus_2x_100e_16b_Poly_Adam_low_wBCE_milestones_warmup+DA_mid
The job ID is 19363138
---------------------------------------------------------
Number of GPUs : 3,6
---------------------------------------------------------
Assert Enviroment modules are loaded...
---------------------------------------------------------
Assert python modules are loaded....
Requirement already satisfied: scikit-learn in /cluster/home/petteed/.local/lib/python3.10/site-packages (1.4.1.post1)
Requirement already satisfied: numpy<2.0,>=1.19.5 in /cluster/apps/eb/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages (from scikit-learn) (1.22.3)
Requirement already satisfied: joblib>=1.2.0 in /cluster/home/petteed/.local/lib/python3.10/site-packages (from scikit-learn) (1.3.2)
Requirement already satisfied: threadpoolctl>=2.0.0 in /cluster/apps/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from scikit-learn) (3.1.0)
Requirement already satisfied: scipy>=1.6.0 in /cluster/apps/eb/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages (from scikit-learn) (1.8.1)
Requirement already satisfied: matplotlib in /cluster/home/petteed/.local/lib/python3.10/site-packages (3.8.3)
Requirement already satisfied: pyparsing>=2.3.1 in /cluster/apps/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from matplotlib) (3.0.8)
Requirement already satisfied: contourpy>=1.0.1 in /cluster/home/petteed/.local/lib/python3.10/site-packages (from matplotlib) (1.2.0)
Requirement already satisfied: packaging>=20.0 in /cluster/apps/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from matplotlib) (20.9)
Requirement already satisfied: fonttools>=4.22.0 in /cluster/home/petteed/.local/lib/python3.10/site-packages (from matplotlib) (4.50.0)
Requirement already satisfied: cycler>=0.10 in /cluster/home/petteed/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)
Requirement already satisfied: kiwisolver>=1.3.1 in /cluster/home/petteed/.local/lib/python3.10/site-packages (from matplotlib) (1.4.5)
Requirement already satisfied: numpy<2,>=1.21 in /cluster/apps/eb/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages (from matplotlib) (1.22.3)
Requirement already satisfied: pillow>=8 in /cluster/home/petteed/.local/lib/python3.10/site-packages (from matplotlib) (10.2.0)
Requirement already satisfied: python-dateutil>=2.7 in /cluster/apps/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from matplotlib) (2.8.2)
Requirement already satisfied: six>=1.5 in /cluster/apps/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)
Requirement already satisfied: cython in /cluster/apps/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (0.29.28)
Collecting git+https://github.com/lucasb-eyer/pydensecrf.git
  Cloning https://github.com/lucasb-eyer/pydensecrf.git to /tmp/pip-req-build-_l0ht6_7
  Resolved https://github.com/lucasb-eyer/pydensecrf.git to commit 2723c7fa4f2ead16ae1ce3d8afe977724bb8f87f
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
---------------------------------------------------------
GPU specifications:
Sun May  5 16:44:38 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-PCIE-40GB          On  | 00000000:48:00.0 Off |                    0 |
| N/A   36C    P0              32W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-PCIE-40GB          On  | 00000000:8A:00.0 Off |                    0 |
| N/A   26C    P0              32W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
Training model:
---------------------------------------------------------------------------------------------------
Enviroment:
---------------------------------------------------------------------------------------------------
TensorFlow version: 2.11.0
GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]
---------------------------------------------------------------------------------------------------
Number of distributed devices: 2
---------------------------------------------------------------------------------------------------
Data shape
Image shape: (16, 512, 512, 3)
Mask shape: (16, 512, 512, 5)
Training dataset:  <BatchDataset element_spec=(TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 512, 512, 5), dtype=tf.float32, name=None))>
Validation dataset: <BatchDataset element_spec=(TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 512, 512, 5), dtype=tf.float32, name=None))>
---------------------------------------------------------------------------------------------------
Details:
---------------------------------------------------------------------------------------------------
Classes:  5
Global batch size: 16
Epochs:  50
Optimizer:  <class 'keras.optimizers.optimizer_experimental.adam.Adam'>
Initial learning rate:  1e-06
Base learning rate:  0.0001
Warmup-batches:  20
Milestones:  [10, 25, 40]
BCE weights:  [2.12, 35.52, 3.31, 12.87, 28.13]
---------------------------------------------------------------------------------------------------
Epoch 1/50

Epoch 1: val_loss improved from inf to 16.95741, saving model to ./models/UNETplus_2x_100e_16b_Poly_Adam_low_wBCE_milestones_warmup+DA_mid
467/467 - 526s - loss: 42.1980 - accuracy: 0.5991 - val_loss: 16.9574 - val_accuracy: 0.6002 - 526s/epoch - 1s/step
Epoch 2/50

Epoch 2: val_loss improved from 16.95741 to 13.35524, saving model to ./models/UNETplus_2x_100e_16b_Poly_Adam_low_wBCE_milestones_warmup+DA_mid

Epoch 1: Learning rate was 9.81982e-05.
467/467 - 415s - loss: 10.4949 - accuracy: 0.6600 - val_loss: 13.3552 - val_accuracy: 0.5840 - 415s/epoch - 889ms/step
Epoch 3/50

Epoch 3: val_loss improved from 13.35524 to 12.56665, saving model to ./models/UNETplus_2x_100e_16b_Poly_Adam_low_wBCE_milestones_warmup+DA_mid

Epoch 2: Learning rate was 9.63927e-05.
467/467 - 421s - loss: 9.5725 - accuracy: 0.6658 - val_loss: 12.5666 - val_accuracy: 0.6186 - 421s/epoch - 902ms/step
Epoch 4/50

Epoch 4: val_loss did not improve from 12.56665

Epoch 3: Learning rate was 9.45834e-05.
467/467 - 400s - loss: 9.1857 - accuracy: 0.6810 - val_loss: 14.0297 - val_accuracy: 0.6205 - 400s/epoch - 858ms/step
Epoch 5/50

Epoch 5: val_loss improved from 12.56665 to 11.65931, saving model to ./models/UNETplus_2x_100e_16b_Poly_Adam_low_wBCE_milestones_warmup+DA_mid

Epoch 4: Learning rate was 9.27703e-05.
467/467 - 415s - loss: 8.8101 - accuracy: 0.6960 - val_loss: 11.6593 - val_accuracy: 0.6352 - 415s/epoch - 889ms/step
Epoch 6/50

Epoch 6: val_loss improved from 11.65931 to 10.06401, saving model to ./models/UNETplus_2x_100e_16b_Poly_Adam_low_wBCE_milestones_warmup+DA_mid

Epoch 5: Learning rate was 9.09533e-05.
467/467 - 419s - loss: 8.5242 - accuracy: 0.7088 - val_loss: 10.0640 - val_accuracy: 0.6554 - 419s/epoch - 897ms/step
Epoch 7/50

Epoch 7: val_loss did not improve from 10.06401

Epoch 6: Learning rate was 8.91322e-05.
467/467 - 400s - loss: 8.2853 - accuracy: 0.7192 - val_loss: 10.7228 - val_accuracy: 0.6471 - 400s/epoch - 858ms/step
Epoch 8/50

Epoch 8: val_loss did not improve from 10.06401

Epoch 7: Learning rate was 8.73069e-05.
467/467 - 399s - loss: 7.9950 - accuracy: 0.7271 - val_loss: 10.4383 - val_accuracy: 0.6896 - 399s/epoch - 855ms/step
Epoch 9/50

Epoch 9: val_loss did not improve from 10.06401

Epoch 8: Learning rate was 8.54774e-05.
467/467 - 399s - loss: 7.7113 - accuracy: 0.7394 - val_loss: 10.6839 - val_accuracy: 0.6808 - 399s/epoch - 855ms/step
Epoch 10/50

Epoch 10: val_loss improved from 10.06401 to 9.41833, saving model to ./models/UNETplus_2x_100e_16b_Poly_Adam_low_wBCE_milestones_warmup+DA_mid

Epoch 9: Learning rate was 8.36436e-05.
467/467 - 411s - loss: 7.4860 - accuracy: 0.7440 - val_loss: 9.4183 - val_accuracy: 0.7273 - 411s/epoch - 880ms/step
Epoch 11/50

Epoch 11: val_loss improved from 9.41833 to 9.15317, saving model to ./models/UNETplus_2x_100e_16b_Poly_Adam_low_wBCE_milestones_warmup+DA_mid

Epoch 10: Learning rate was 8.18052e-06.
467/467 - 413s - loss: 7.2647 - accuracy: 0.7553 - val_loss: 9.1532 - val_accuracy: 0.7428 - 413s/epoch - 885ms/step
Epoch 12/50

Epoch 12: val_loss improved from 9.15317 to 7.15349, saving model to ./models/UNETplus_2x_100e_16b_Poly_Adam_low_wBCE_milestones_warmup+DA_mid

Epoch 11: Learning rate was 7.99623e-06.
467/467 - 417s - loss: 6.7540 - accuracy: 0.7741 - val_loss: 7.1535 - val_accuracy: 0.7680 - 417s/epoch - 892ms/step
Epoch 13/50

Epoch 13: val_loss did not improve from 7.15349

Epoch 12: Learning rate was 7.81146e-06.
467/467 - 411s - loss: 6.2317 - accuracy: 0.7807 - val_loss: 7.2676 - val_accuracy: 0.7739 - 411s/epoch - 880ms/step
Epoch 14/50

Epoch 14: val_loss improved from 7.15349 to 6.95378, saving model to ./models/UNETplus_2x_100e_16b_Poly_Adam_low_wBCE_milestones_warmup+DA_mid

Epoch 13: Learning rate was 7.62621e-06.
467/467 - 427s - loss: 6.0184 - accuracy: 0.7886 - val_loss: 6.9538 - val_accuracy: 0.7765 - 427s/epoch - 915ms/step
Epoch 15/50

Epoch 15: val_loss improved from 6.95378 to 6.83021, saving model to ./models/UNETplus_2x_100e_16b_Poly_Adam_low_wBCE_milestones_warmup+DA_mid

Epoch 14: Learning rate was 7.44045e-06.
467/467 - 425s - loss: 5.8171 - accuracy: 0.7981 - val_loss: 6.8302 - val_accuracy: 0.7706 - 425s/epoch - 910ms/step
Epoch 16/50

Epoch 16: val_loss did not improve from 6.83021

Epoch 15: Learning rate was 7.25418e-06.
467/467 - 407s - loss: 5.7053 - accuracy: 0.8023 - val_loss: 7.0462 - val_accuracy: 0.7763 - 407s/epoch - 872ms/step
Epoch 17/50

Epoch 17: val_loss did not improve from 6.83021

Epoch 16: Learning rate was 7.06737e-06.
467/467 - 407s - loss: 5.5380 - accuracy: 0.8114 - val_loss: 6.8726 - val_accuracy: 0.7792 - 407s/epoch - 872ms/step
Epoch 18/50

Epoch 18: val_loss did not improve from 6.83021

Epoch 17: Learning rate was 6.88002e-06.
467/467 - 407s - loss: 5.4072 - accuracy: 0.8165 - val_loss: 6.8571 - val_accuracy: 0.7776 - 407s/epoch - 873ms/step
Epoch 19/50

Epoch 19: val_loss did not improve from 6.83021

Epoch 18: Learning rate was 6.69209e-06.
467/467 - 405s - loss: 5.2761 - accuracy: 0.8226 - val_loss: 7.0431 - val_accuracy: 0.7846 - 405s/epoch - 867ms/step
Epoch 20/50

Epoch 20: val_loss did not improve from 6.83021

Epoch 19: Learning rate was 6.50358e-06.
467/467 - 404s - loss: 5.1101 - accuracy: 0.8278 - val_loss: 6.9397 - val_accuracy: 0.7834 - 404s/epoch - 866ms/step
Epoch 21/50

Epoch 21: val_loss improved from 6.83021 to 6.72567, saving model to ./models/UNETplus_2x_100e_16b_Poly_Adam_low_wBCE_milestones_warmup+DA_mid

Epoch 20: Learning rate was 6.31446e-06.
467/467 - 424s - loss: 4.9760 - accuracy: 0.8355 - val_loss: 6.7257 - val_accuracy: 0.7862 - 424s/epoch - 909ms/step
Epoch 22/50

Epoch 22: val_loss did not improve from 6.72567

Epoch 21: Learning rate was 6.12471e-06.
467/467 - 404s - loss: 4.9387 - accuracy: 0.8371 - val_loss: 7.0161 - val_accuracy: 0.7813 - 404s/epoch - 865ms/step
Epoch 23/50

Epoch 23: val_loss did not improve from 6.72567

Epoch 22: Learning rate was 5.9343e-06.
467/467 - 405s - loss: 4.7674 - accuracy: 0.8449 - val_loss: 7.2233 - val_accuracy: 0.7769 - 405s/epoch - 867ms/step
Epoch 24/50

Epoch 24: val_loss did not improve from 6.72567

Epoch 23: Learning rate was 5.74321e-06.
467/467 - 404s - loss: 4.6871 - accuracy: 0.8486 - val_loss: 7.2585 - val_accuracy: 0.7756 - 404s/epoch - 866ms/step
Epoch 25/50

Epoch 25: val_loss improved from 6.72567 to 6.70004, saving model to ./models/UNETplus_2x_100e_16b_Poly_Adam_low_wBCE_milestones_warmup+DA_mid

Epoch 24: Learning rate was 5.55141e-06.
467/467 - 424s - loss: 4.5942 - accuracy: 0.8519 - val_loss: 6.7000 - val_accuracy: 0.7920 - 424s/epoch - 909ms/step
Epoch 26/50

Epoch 26: val_loss did not improve from 6.70004

Epoch 25: Learning rate was 5.35887e-07.
467/467 - 404s - loss: 4.4869 - accuracy: 0.8553 - val_loss: 7.3720 - val_accuracy: 0.7797 - 404s/epoch - 864ms/step
Epoch 27/50

Epoch 27: val_loss improved from 6.70004 to 6.47129, saving model to ./models/UNETplus_2x_100e_16b_Poly_Adam_low_wBCE_milestones_warmup+DA_mid

Epoch 26: Learning rate was 5.16556e-07.
467/467 - 423s - loss: 4.4771 - accuracy: 0.8528 - val_loss: 6.4713 - val_accuracy: 0.8023 - 423s/epoch - 906ms/step
Epoch 28/50

Epoch 28: val_loss did not improve from 6.47129

Epoch 27: Learning rate was 4.97144e-07.
467/467 - 409s - loss: 4.3596 - accuracy: 0.8588 - val_loss: 6.5396 - val_accuracy: 0.8018 - 409s/epoch - 876ms/step
Epoch 29/50

Epoch 29: val_loss improved from 6.47129 to 6.33753, saving model to ./models/UNETplus_2x_100e_16b_Poly_Adam_low_wBCE_milestones_warmup+DA_mid

Epoch 28: Learning rate was 4.77647e-07.
467/467 - 428s - loss: 4.3628 - accuracy: 0.8591 - val_loss: 6.3375 - val_accuracy: 0.8010 - 428s/epoch - 916ms/step
Epoch 30/50

Epoch 30: val_loss improved from 6.33753 to 6.17655, saving model to ./models/UNETplus_2x_100e_16b_Poly_Adam_low_wBCE_milestones_warmup+DA_mid

Epoch 29: Learning rate was 4.58062e-07.
467/467 - 424s - loss: 4.2980 - accuracy: 0.8613 - val_loss: 6.1765 - val_accuracy: 0.8021 - 424s/epoch - 908ms/step
Epoch 31/50

Epoch 31: val_loss did not improve from 6.17655

Epoch 30: Learning rate was 4.38383e-07.
467/467 - 401s - loss: 4.2724 - accuracy: 0.8624 - val_loss: 6.3807 - val_accuracy: 0.8058 - 401s/epoch - 858ms/step
Epoch 32/50

Epoch 32: val_loss did not improve from 6.17655

Epoch 31: Learning rate was 4.18606e-07.
467/467 - 399s - loss: 4.3146 - accuracy: 0.8607 - val_loss: 6.2483 - val_accuracy: 0.7995 - 399s/epoch - 854ms/step
Epoch 33/50

Epoch 33: val_loss did not improve from 6.17655

Epoch 32: Learning rate was 3.98724e-07.
467/467 - 401s - loss: 4.2459 - accuracy: 0.8630 - val_loss: 6.3097 - val_accuracy: 0.8028 - 401s/epoch - 858ms/step
Epoch 34/50

Epoch 34: val_loss did not improve from 6.17655

Epoch 33: Learning rate was 3.78731e-07.
467/467 - 401s - loss: 4.2542 - accuracy: 0.8641 - val_loss: 6.3039 - val_accuracy: 0.8017 - 401s/epoch - 858ms/step
Epoch 35/50

Epoch 35: val_loss did not improve from 6.17655

Epoch 34: Learning rate was 3.5862e-07.
467/467 - 401s - loss: 4.2529 - accuracy: 0.8642 - val_loss: 6.2981 - val_accuracy: 0.8042 - 401s/epoch - 859ms/step
Epoch 36/50

Epoch 36: val_loss did not improve from 6.17655

Epoch 35: Learning rate was 3.38383e-07.
467/467 - 401s - loss: 4.2235 - accuracy: 0.8658 - val_loss: 6.2844 - val_accuracy: 0.7993 - 401s/epoch - 860ms/step
Epoch 37/50

Epoch 37: val_loss did not improve from 6.17655

Epoch 36: Learning rate was 3.18011e-07.
467/467 - 404s - loss: 4.2462 - accuracy: 0.8643 - val_loss: 6.3519 - val_accuracy: 0.8052 - 404s/epoch - 864ms/step
Epoch 38/50

Epoch 38: val_loss did not improve from 6.17655

Epoch 37: Learning rate was 2.97492e-07.
467/467 - 405s - loss: 4.2272 - accuracy: 0.8652 - val_loss: 6.3464 - val_accuracy: 0.8034 - 405s/epoch - 867ms/step
Epoch 39/50

Epoch 39: val_loss did not improve from 6.17655

Epoch 38: Learning rate was 2.76815e-07.
467/467 - 402s - loss: 4.2365 - accuracy: 0.8641 - val_loss: 6.2126 - val_accuracy: 0.8091 - 402s/epoch - 861ms/step
Epoch 40/50

Epoch 40: val_loss did not improve from 6.17655

Epoch 39: Learning rate was 2.55965e-07.
467/467 - 405s - loss: 4.1921 - accuracy: 0.8665 - val_loss: 6.3910 - val_accuracy: 0.8031 - 405s/epoch - 868ms/step
Epoch 41/50

Epoch 41: val_loss did not improve from 6.17655

Epoch 40: Learning rate was 2.34924e-08.
467/467 - 404s - loss: 4.2077 - accuracy: 0.8670 - val_loss: 6.3118 - val_accuracy: 0.8042 - 404s/epoch - 865ms/step
Epoch 42/50

Epoch 42: val_loss did not improve from 6.17655

Epoch 41: Learning rate was 2.13671e-08.
467/467 - 402s - loss: 4.1827 - accuracy: 0.8663 - val_loss: 6.3217 - val_accuracy: 0.8063 - 402s/epoch - 862ms/step
Epoch 43/50

Epoch 43: val_loss did not improve from 6.17655

Epoch 42: Learning rate was 1.9218e-08.
467/467 - 403s - loss: 4.2132 - accuracy: 0.8654 - val_loss: 6.5377 - val_accuracy: 0.8036 - 403s/epoch - 863ms/step
Epoch 44/50

Epoch 44: val_loss did not improve from 6.17655

Epoch 43: Learning rate was 1.70418e-08.
467/467 - 407s - loss: 4.1814 - accuracy: 0.8675 - val_loss: 6.1926 - val_accuracy: 0.8025 - 407s/epoch - 872ms/step
Epoch 45/50

Epoch 45: val_loss improved from 6.17655 to 6.11742, saving model to ./models/UNETplus_2x_100e_16b_Poly_Adam_low_wBCE_milestones_warmup+DA_mid

Epoch 44: Learning rate was 1.48342e-08.
467/467 - 418s - loss: 4.1713 - accuracy: 0.8673 - val_loss: 6.1174 - val_accuracy: 0.8084 - 418s/epoch - 894ms/step
Epoch 46/50

Epoch 46: val_loss did not improve from 6.11742

Epoch 45: Learning rate was 1.25893e-08.
467/467 - 402s - loss: 4.1493 - accuracy: 0.8681 - val_loss: 6.1290 - val_accuracy: 0.8036 - 402s/epoch - 860ms/step
Epoch 47/50

Epoch 47: val_loss did not improve from 6.11742

Epoch 46: Learning rate was 1.02987e-08.
467/467 - 402s - loss: 4.1957 - accuracy: 0.8666 - val_loss: 6.4060 - val_accuracy: 0.8046 - 402s/epoch - 861ms/step
Epoch 48/50

Epoch 48: val_loss did not improve from 6.11742

Epoch 47: Learning rate was 7.94943e-09.
467/467 - 402s - loss: 4.1959 - accuracy: 0.8661 - val_loss: 6.3555 - val_accuracy: 0.8039 - 402s/epoch - 861ms/step
Epoch 49/50

Epoch 49: val_loss did not improve from 6.11742

Epoch 48: Learning rate was 5.51892e-09.
467/467 - 400s - loss: 4.1538 - accuracy: 0.8681 - val_loss: 6.2511 - val_accuracy: 0.8019 - 400s/epoch - 856ms/step
Epoch 50/50

Epoch 50: val_loss improved from 6.11742 to 6.06370, saving model to ./models/UNETplus_2x_100e_16b_Poly_Adam_low_wBCE_milestones_warmup+DA_mid

Epoch 49: Learning rate was 2.95752e-09.
467/467 - 419s - loss: 4.1788 - accuracy: 0.8669 - val_loss: 6.0637 - val_accuracy: 0.8033 - 419s/epoch - 898ms/step
---------------------------------------------------------
Script completed
