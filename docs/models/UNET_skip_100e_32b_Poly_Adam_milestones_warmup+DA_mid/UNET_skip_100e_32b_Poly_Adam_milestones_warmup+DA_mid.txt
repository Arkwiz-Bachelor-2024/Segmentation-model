We are running from this directory: /cluster/work/petteed/seg_model/Segmentation-model
---------------------------------------------------------
The name of the job is: UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid
The job ID is 19344234
---------------------------------------------------------
Number of GPUs : 6,9
---------------------------------------------------------
Assert Enviroment modules are loaded...
---------------------------------------------------------
Assert python modules are loaded....
Requirement already satisfied: scikit-learn in /cluster/home/petteed/.local/lib/python3.10/site-packages (1.4.1.post1)
Requirement already satisfied: scipy>=1.6.0 in /cluster/apps/eb/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages (from scikit-learn) (1.8.1)
Requirement already satisfied: numpy<2.0,>=1.19.5 in /cluster/apps/eb/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages (from scikit-learn) (1.22.3)
Requirement already satisfied: joblib>=1.2.0 in /cluster/home/petteed/.local/lib/python3.10/site-packages (from scikit-learn) (1.3.2)
Requirement already satisfied: threadpoolctl>=2.0.0 in /cluster/apps/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from scikit-learn) (3.1.0)
Requirement already satisfied: matplotlib in /cluster/home/petteed/.local/lib/python3.10/site-packages (3.8.3)
Requirement already satisfied: fonttools>=4.22.0 in /cluster/home/petteed/.local/lib/python3.10/site-packages (from matplotlib) (4.50.0)
Requirement already satisfied: numpy<2,>=1.21 in /cluster/apps/eb/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages (from matplotlib) (1.22.3)
Requirement already satisfied: pyparsing>=2.3.1 in /cluster/apps/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from matplotlib) (3.0.8)
Requirement already satisfied: packaging>=20.0 in /cluster/apps/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from matplotlib) (20.9)
Requirement already satisfied: python-dateutil>=2.7 in /cluster/apps/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from matplotlib) (2.8.2)
Requirement already satisfied: cycler>=0.10 in /cluster/home/petteed/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)
Requirement already satisfied: pillow>=8 in /cluster/home/petteed/.local/lib/python3.10/site-packages (from matplotlib) (10.2.0)
Requirement already satisfied: kiwisolver>=1.3.1 in /cluster/home/petteed/.local/lib/python3.10/site-packages (from matplotlib) (1.4.5)
Requirement already satisfied: contourpy>=1.0.1 in /cluster/home/petteed/.local/lib/python3.10/site-packages (from matplotlib) (1.2.0)
Requirement already satisfied: six>=1.5 in /cluster/apps/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)
Requirement already satisfied: cython in /cluster/apps/eb/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (0.29.28)
Collecting git+https://github.com/lucasb-eyer/pydensecrf.git
  Cloning https://github.com/lucasb-eyer/pydensecrf.git to /tmp/pip-req-build-clqybusk
  Resolved https://github.com/lucasb-eyer/pydensecrf.git to commit 2723c7fa4f2ead16ae1ce3d8afe977724bb8f87f
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
---------------------------------------------------------
GPU specifications:
Wed May  1 00:12:43 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-PCIE-40GB          On  | 00000000:8A:00.0 Off |                    0 |
| N/A   41C    P0              36W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-PCIE-40GB          On  | 00000000:C2:00.0 Off |                    0 |
| N/A   25C    P0              31W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
Training model:
---------------------------------------------------------------------------------------------------
Enviroment:
---------------------------------------------------------------------------------------------------
TensorFlow version: 2.11.0
GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
Data shape
Image shape: (32, 512, 512, 3)
Mask shape: (32, 512, 512, 1)
Training dataset:  <BatchDataset element_spec=(TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 512, 512, 1), dtype=tf.uint8, name=None))>
Validation dataset: <BatchDataset element_spec=(TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 512, 512, 1), dtype=tf.uint8, name=None))>
---------------------------------------------------------------------------------------------------
Details:
---------------------------------------------------------------------------------------------------
Classes:  5
Global batch size: 32
Epochs:  100
Optimizer:  <class 'keras.optimizers.optimizer_experimental.adam.Adam'>
Initial learning rate:  0.0001
Base learning rate:  0.01
Warmup-batches:  20
Milestones:  [10, 30, 60]
Tvernsky weights :  [(2, 1), (1, 3), (2, 1), (1, 3), (1, 3)]
BCE weights:  [2.12, 35.52, 3.31, 12.87, 28.13]
---------------------------------------------------------------------------------------------------
Epoch 1/100

Epoch 1: val_loss improved from inf to 1.84423, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid
234/234 - 723s - loss: 1.0490 - accuracy: 0.5788 - mean_io_u: 0.1568 - val_loss: 1.8442 - val_accuracy: 0.6002 - val_mean_io_u: 0.1200 - 723s/epoch - 3s/step
Epoch 2/100

Epoch 2: val_loss improved from 1.84423 to 0.90060, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 1: Learning rate was 0.00990995.
234/234 - 373s - loss: 0.9909 - accuracy: 0.5420 - mean_io_u: 0.1408 - val_loss: 0.9006 - val_accuracy: 0.6380 - val_mean_io_u: 0.1589 - 373s/epoch - 2s/step
Epoch 3/100

Epoch 3: val_loss did not improve from 0.90060

Epoch 2: Learning rate was 0.00981982.
234/234 - 357s - loss: 0.9694 - accuracy: 0.5705 - mean_io_u: 0.1494 - val_loss: 0.9212 - val_accuracy: 0.6002 - val_mean_io_u: 0.1200 - 357s/epoch - 2s/step
Epoch 4/100

Epoch 4: val_loss did not improve from 0.90060

Epoch 3: Learning rate was 0.00972959.
234/234 - 356s - loss: 0.9512 - accuracy: 0.5792 - mean_io_u: 0.1479 - val_loss: 0.9514 - val_accuracy: 0.6292 - val_mean_io_u: 0.1985 - 356s/epoch - 2s/step
Epoch 5/100

Epoch 5: val_loss improved from 0.90060 to 0.88709, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 4: Learning rate was 0.00963927.
234/234 - 374s - loss: 0.9512 - accuracy: 0.5833 - mean_io_u: 0.1443 - val_loss: 0.8871 - val_accuracy: 0.6500 - val_mean_io_u: 0.1808 - 374s/epoch - 2s/step
Epoch 6/100

Epoch 6: val_loss improved from 0.88709 to 0.85622, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 5: Learning rate was 0.00954885.
234/234 - 379s - loss: 0.9462 - accuracy: 0.5986 - mean_io_u: 0.1514 - val_loss: 0.8562 - val_accuracy: 0.6276 - val_mean_io_u: 0.1484 - 379s/epoch - 2s/step
Epoch 7/100

Epoch 7: val_loss did not improve from 0.85622

Epoch 6: Learning rate was 0.00945834.
234/234 - 356s - loss: 0.9395 - accuracy: 0.5816 - mean_io_u: 0.1423 - val_loss: 0.8846 - val_accuracy: 0.6329 - val_mean_io_u: 0.1614 - 356s/epoch - 2s/step
Epoch 8/100

Epoch 8: val_loss did not improve from 0.85622

Epoch 7: Learning rate was 0.00936774.
234/234 - 356s - loss: 0.9396 - accuracy: 0.5763 - mean_io_u: 0.1410 - val_loss: 0.9734 - val_accuracy: 0.6002 - val_mean_io_u: 0.1200 - 356s/epoch - 2s/step
Epoch 9/100

Epoch 9: val_loss did not improve from 0.85622

Epoch 8: Learning rate was 0.00927703.
234/234 - 352s - loss: 0.9362 - accuracy: 0.5725 - mean_io_u: 0.1304 - val_loss: 1.2400 - val_accuracy: 0.6002 - val_mean_io_u: 0.1200 - 352s/epoch - 2s/step
Epoch 10/100

Epoch 10: val_loss did not improve from 0.85622

Epoch 9: Learning rate was 0.00918623.
234/234 - 355s - loss: 0.9330 - accuracy: 0.5612 - mean_io_u: 0.1233 - val_loss: 0.8769 - val_accuracy: 0.6002 - val_mean_io_u: 0.1200 - 355s/epoch - 2s/step
Epoch 11/100

Epoch 11: val_loss did not improve from 0.85622

Epoch 10: Learning rate was 0.000909533.
234/234 - 354s - loss: 0.9345 - accuracy: 0.5594 - mean_io_u: 0.1262 - val_loss: 0.8671 - val_accuracy: 0.6002 - val_mean_io_u: 0.1200 - 354s/epoch - 2s/step
Epoch 12/100

Epoch 12: val_loss improved from 0.85622 to 0.85540, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 11: Learning rate was 0.000900432.
234/234 - 372s - loss: 0.9130 - accuracy: 0.5803 - mean_io_u: 0.1268 - val_loss: 0.8554 - val_accuracy: 0.6260 - val_mean_io_u: 0.1480 - 372s/epoch - 2s/step
Epoch 13/100

Epoch 13: val_loss improved from 0.85540 to 0.85297, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 12: Learning rate was 0.000891322.
234/234 - 377s - loss: 0.9002 - accuracy: 0.5833 - mean_io_u: 0.1456 - val_loss: 0.8530 - val_accuracy: 0.6454 - val_mean_io_u: 0.1732 - 377s/epoch - 2s/step
Epoch 14/100

Epoch 14: val_loss improved from 0.85297 to 0.84823, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 13: Learning rate was 0.000882201.
234/234 - 380s - loss: 0.8965 - accuracy: 0.5891 - mean_io_u: 0.1569 - val_loss: 0.8482 - val_accuracy: 0.6528 - val_mean_io_u: 0.1827 - 380s/epoch - 2s/step
Epoch 15/100

Epoch 15: val_loss did not improve from 0.84823

Epoch 14: Learning rate was 0.000873069.
234/234 - 354s - loss: 0.8943 - accuracy: 0.5928 - mean_io_u: 0.1624 - val_loss: 0.8484 - val_accuracy: 0.6467 - val_mean_io_u: 0.1766 - 354s/epoch - 2s/step
Epoch 16/100

Epoch 16: val_loss did not improve from 0.84823

Epoch 15: Learning rate was 0.000863927.
234/234 - 357s - loss: 0.8937 - accuracy: 0.5943 - mean_io_u: 0.1645 - val_loss: 0.8505 - val_accuracy: 0.6487 - val_mean_io_u: 0.1837 - 357s/epoch - 2s/step
Epoch 17/100

Epoch 17: val_loss improved from 0.84823 to 0.84134, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 16: Learning rate was 0.000854774.
234/234 - 373s - loss: 0.8914 - accuracy: 0.5969 - mean_io_u: 0.1672 - val_loss: 0.8413 - val_accuracy: 0.6545 - val_mean_io_u: 0.1886 - 373s/epoch - 2s/step
Epoch 18/100

Epoch 18: val_loss did not improve from 0.84134

Epoch 17: Learning rate was 0.00084561.
234/234 - 354s - loss: 0.8887 - accuracy: 0.6007 - mean_io_u: 0.1699 - val_loss: 0.8473 - val_accuracy: 0.6546 - val_mean_io_u: 0.1879 - 354s/epoch - 2s/step
Epoch 19/100

Epoch 19: val_loss improved from 0.84134 to 0.83494, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 18: Learning rate was 0.000836436.
234/234 - 379s - loss: 0.8856 - accuracy: 0.6042 - mean_io_u: 0.1732 - val_loss: 0.8349 - val_accuracy: 0.6526 - val_mean_io_u: 0.1934 - 379s/epoch - 2s/step
Epoch 20/100

Epoch 20: val_loss did not improve from 0.83494

Epoch 19: Learning rate was 0.00082725.
234/234 - 356s - loss: 0.8856 - accuracy: 0.6065 - mean_io_u: 0.1750 - val_loss: 0.8430 - val_accuracy: 0.6524 - val_mean_io_u: 0.1888 - 356s/epoch - 2s/step
Epoch 21/100

Epoch 21: val_loss did not improve from 0.83494

Epoch 20: Learning rate was 0.000818052.
234/234 - 356s - loss: 0.8843 - accuracy: 0.6068 - mean_io_u: 0.1761 - val_loss: 0.8391 - val_accuracy: 0.6542 - val_mean_io_u: 0.1983 - 356s/epoch - 2s/step
Epoch 22/100

Epoch 22: val_loss did not improve from 0.83494

Epoch 21: Learning rate was 0.000808843.
234/234 - 358s - loss: 0.8768 - accuracy: 0.6116 - mean_io_u: 0.1786 - val_loss: 0.8530 - val_accuracy: 0.6590 - val_mean_io_u: 0.1990 - 358s/epoch - 2s/step
Epoch 23/100

Epoch 23: val_loss did not improve from 0.83494

Epoch 22: Learning rate was 0.000799623.
234/234 - 357s - loss: 0.8749 - accuracy: 0.6136 - mean_io_u: 0.1804 - val_loss: 0.8653 - val_accuracy: 0.6662 - val_mean_io_u: 0.2096 - 357s/epoch - 2s/step
Epoch 24/100

Epoch 24: val_loss did not improve from 0.83494

Epoch 23: Learning rate was 0.00079039.
234/234 - 353s - loss: 0.8675 - accuracy: 0.6246 - mean_io_u: 0.1862 - val_loss: 0.8497 - val_accuracy: 0.6584 - val_mean_io_u: 0.2075 - 353s/epoch - 2s/step
Epoch 25/100

Epoch 25: val_loss improved from 0.83494 to 0.82737, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 24: Learning rate was 0.000781146.
234/234 - 377s - loss: 0.8666 - accuracy: 0.6319 - mean_io_u: 0.1908 - val_loss: 0.8274 - val_accuracy: 0.6718 - val_mean_io_u: 0.2120 - 377s/epoch - 2s/step
Epoch 26/100

Epoch 26: val_loss improved from 0.82737 to 0.81365, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 25: Learning rate was 0.00077189.
234/234 - 378s - loss: 0.8583 - accuracy: 0.6345 - mean_io_u: 0.1925 - val_loss: 0.8137 - val_accuracy: 0.6769 - val_mean_io_u: 0.2145 - 378s/epoch - 2s/step
Epoch 27/100

Epoch 27: val_loss did not improve from 0.81365

Epoch 26: Learning rate was 0.000762621.
234/234 - 356s - loss: 0.8556 - accuracy: 0.6406 - mean_io_u: 0.1958 - val_loss: 0.8227 - val_accuracy: 0.6705 - val_mean_io_u: 0.2100 - 356s/epoch - 2s/step
Epoch 28/100

Epoch 28: val_loss improved from 0.81365 to 0.81091, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 27: Learning rate was 0.000753339.
234/234 - 378s - loss: 0.8495 - accuracy: 0.6447 - mean_io_u: 0.1977 - val_loss: 0.8109 - val_accuracy: 0.6827 - val_mean_io_u: 0.2176 - 378s/epoch - 2s/step
Epoch 29/100

Epoch 29: val_loss improved from 0.81091 to 0.80961, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 28: Learning rate was 0.000744045.
234/234 - 377s - loss: 0.8445 - accuracy: 0.6513 - mean_io_u: 0.2010 - val_loss: 0.8096 - val_accuracy: 0.6823 - val_mean_io_u: 0.2181 - 377s/epoch - 2s/step
Epoch 30/100

Epoch 30: val_loss did not improve from 0.80961

Epoch 29: Learning rate was 0.000734738.
234/234 - 356s - loss: 0.8419 - accuracy: 0.6583 - mean_io_u: 0.2045 - val_loss: 0.8281 - val_accuracy: 0.6750 - val_mean_io_u: 0.2153 - 356s/epoch - 2s/step
Epoch 31/100

Epoch 31: val_loss improved from 0.80961 to 0.80925, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 30: Learning rate was 7.25418e-05.
234/234 - 378s - loss: 0.8363 - accuracy: 0.6658 - mean_io_u: 0.2079 - val_loss: 0.8093 - val_accuracy: 0.6901 - val_mean_io_u: 0.2211 - 378s/epoch - 2s/step
Epoch 32/100

Epoch 32: val_loss improved from 0.80925 to 0.78481, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 31: Learning rate was 7.16084e-05.
234/234 - 372s - loss: 0.8225 - accuracy: 0.6808 - mean_io_u: 0.2143 - val_loss: 0.7848 - val_accuracy: 0.6999 - val_mean_io_u: 0.2220 - 372s/epoch - 2s/step
Epoch 33/100

Epoch 33: val_loss improved from 0.78481 to 0.78237, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 32: Learning rate was 7.06737e-05.
234/234 - 379s - loss: 0.8121 - accuracy: 0.6902 - mean_io_u: 0.2195 - val_loss: 0.7824 - val_accuracy: 0.7010 - val_mean_io_u: 0.2241 - 379s/epoch - 2s/step
Epoch 34/100

Epoch 34: val_loss improved from 0.78237 to 0.77972, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 33: Learning rate was 6.97377e-05.
234/234 - 372s - loss: 0.8081 - accuracy: 0.6925 - mean_io_u: 0.2207 - val_loss: 0.7797 - val_accuracy: 0.7076 - val_mean_io_u: 0.2278 - 372s/epoch - 2s/step
Epoch 35/100

Epoch 35: val_loss improved from 0.77972 to 0.77957, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 34: Learning rate was 6.88002e-05.
234/234 - 377s - loss: 0.8041 - accuracy: 0.6928 - mean_io_u: 0.2205 - val_loss: 0.7796 - val_accuracy: 0.7051 - val_mean_io_u: 0.2265 - 377s/epoch - 2s/step
Epoch 36/100

Epoch 36: val_loss improved from 0.77957 to 0.77649, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 35: Learning rate was 6.78613e-05.
234/234 - 375s - loss: 0.8049 - accuracy: 0.6947 - mean_io_u: 0.2217 - val_loss: 0.7765 - val_accuracy: 0.7083 - val_mean_io_u: 0.2288 - 375s/epoch - 2s/step
Epoch 37/100

Epoch 37: val_loss improved from 0.77649 to 0.77113, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 36: Learning rate was 6.69209e-05.
234/234 - 374s - loss: 0.7990 - accuracy: 0.6970 - mean_io_u: 0.2228 - val_loss: 0.7711 - val_accuracy: 0.7132 - val_mean_io_u: 0.2303 - 374s/epoch - 2s/step
Epoch 38/100

Epoch 38: val_loss did not improve from 0.77113

Epoch 37: Learning rate was 6.59791e-05.
234/234 - 356s - loss: 0.7983 - accuracy: 0.6988 - mean_io_u: 0.2239 - val_loss: 0.7761 - val_accuracy: 0.7133 - val_mean_io_u: 0.2302 - 356s/epoch - 2s/step
Epoch 39/100

Epoch 39: val_loss did not improve from 0.77113

Epoch 38: Learning rate was 6.50358e-05.
234/234 - 355s - loss: 0.7949 - accuracy: 0.7034 - mean_io_u: 0.2263 - val_loss: 0.7759 - val_accuracy: 0.7121 - val_mean_io_u: 0.2295 - 355s/epoch - 2s/step
Epoch 40/100

Epoch 40: val_loss did not improve from 0.77113

Epoch 39: Learning rate was 6.4091e-05.
234/234 - 356s - loss: 0.7916 - accuracy: 0.7063 - mean_io_u: 0.2282 - val_loss: 0.7721 - val_accuracy: 0.7175 - val_mean_io_u: 0.2321 - 356s/epoch - 2s/step
Epoch 41/100

Epoch 41: val_loss improved from 0.77113 to 0.76667, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 40: Learning rate was 6.31446e-05.
234/234 - 374s - loss: 0.7896 - accuracy: 0.7072 - mean_io_u: 0.2288 - val_loss: 0.7667 - val_accuracy: 0.7184 - val_mean_io_u: 0.2327 - 374s/epoch - 2s/step
Epoch 42/100

Epoch 42: val_loss did not improve from 0.76667

Epoch 41: Learning rate was 6.21966e-05.
234/234 - 353s - loss: 0.7874 - accuracy: 0.7088 - mean_io_u: 0.2296 - val_loss: 0.7778 - val_accuracy: 0.7130 - val_mean_io_u: 0.2302 - 353s/epoch - 2s/step
Epoch 43/100

Epoch 43: val_loss did not improve from 0.76667

Epoch 42: Learning rate was 6.12471e-05.
234/234 - 352s - loss: 0.7845 - accuracy: 0.7150 - mean_io_u: 0.2328 - val_loss: 0.7682 - val_accuracy: 0.7213 - val_mean_io_u: 0.2345 - 352s/epoch - 2s/step
Epoch 44/100

Epoch 44: val_loss did not improve from 0.76667

Epoch 43: Learning rate was 6.02958e-05.
234/234 - 354s - loss: 0.7822 - accuracy: 0.7164 - mean_io_u: 0.2335 - val_loss: 0.7721 - val_accuracy: 0.7184 - val_mean_io_u: 0.2328 - 354s/epoch - 2s/step
Epoch 45/100

Epoch 45: val_loss did not improve from 0.76667

Epoch 44: Learning rate was 5.9343e-05.
234/234 - 356s - loss: 0.7784 - accuracy: 0.7184 - mean_io_u: 0.2350 - val_loss: 0.7706 - val_accuracy: 0.7224 - val_mean_io_u: 0.2349 - 356s/epoch - 2s/step
Epoch 46/100

Epoch 46: val_loss did not improve from 0.76667

Epoch 45: Learning rate was 5.83884e-05.
234/234 - 351s - loss: 0.7795 - accuracy: 0.7167 - mean_io_u: 0.2341 - val_loss: 0.7720 - val_accuracy: 0.7152 - val_mean_io_u: 0.2307 - 351s/epoch - 2s/step
Epoch 47/100

Epoch 47: val_loss did not improve from 0.76667

Epoch 46: Learning rate was 5.74321e-05.
234/234 - 354s - loss: 0.7747 - accuracy: 0.7226 - mean_io_u: 0.2374 - val_loss: 0.7694 - val_accuracy: 0.7200 - val_mean_io_u: 0.2335 - 354s/epoch - 2s/step
Epoch 48/100

Epoch 48: val_loss improved from 0.76667 to 0.76313, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 47: Learning rate was 5.6474e-05.
234/234 - 377s - loss: 0.7760 - accuracy: 0.7224 - mean_io_u: 0.2373 - val_loss: 0.7631 - val_accuracy: 0.7228 - val_mean_io_u: 0.2357 - 377s/epoch - 2s/step
Epoch 49/100

Epoch 49: val_loss did not improve from 0.76313

Epoch 48: Learning rate was 5.55141e-05.
234/234 - 355s - loss: 0.7744 - accuracy: 0.7209 - mean_io_u: 0.2367 - val_loss: 0.7672 - val_accuracy: 0.7209 - val_mean_io_u: 0.2341 - 355s/epoch - 2s/step
Epoch 50/100

Epoch 50: val_loss improved from 0.76313 to 0.76236, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 49: Learning rate was 5.45523e-05.
234/234 - 376s - loss: 0.7690 - accuracy: 0.7251 - mean_io_u: 0.2389 - val_loss: 0.7624 - val_accuracy: 0.7215 - val_mean_io_u: 0.2340 - 376s/epoch - 2s/step
Epoch 51/100

Epoch 51: val_loss did not improve from 0.76236

Epoch 50: Learning rate was 5.35887e-05.
234/234 - 356s - loss: 0.7713 - accuracy: 0.7239 - mean_io_u: 0.2383 - val_loss: 0.7699 - val_accuracy: 0.7239 - val_mean_io_u: 0.2361 - 356s/epoch - 2s/step
Epoch 52/100

Epoch 52: val_loss improved from 0.76236 to 0.76107, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 51: Learning rate was 5.26231e-05.
234/234 - 378s - loss: 0.7680 - accuracy: 0.7302 - mean_io_u: 0.2418 - val_loss: 0.7611 - val_accuracy: 0.7250 - val_mean_io_u: 0.2355 - 378s/epoch - 2s/step
Epoch 53/100

Epoch 53: val_loss did not improve from 0.76107

Epoch 52: Learning rate was 5.16556e-05.
234/234 - 356s - loss: 0.7660 - accuracy: 0.7310 - mean_io_u: 0.2421 - val_loss: 0.7643 - val_accuracy: 0.7228 - val_mean_io_u: 0.2345 - 356s/epoch - 2s/step
Epoch 54/100

Epoch 54: val_loss did not improve from 0.76107

Epoch 53: Learning rate was 5.0686e-05.
234/234 - 352s - loss: 0.7618 - accuracy: 0.7354 - mean_io_u: 0.2447 - val_loss: 0.7650 - val_accuracy: 0.7244 - val_mean_io_u: 0.2363 - 352s/epoch - 2s/step
Epoch 55/100

Epoch 55: val_loss improved from 0.76107 to 0.76026, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 54: Learning rate was 4.97144e-05.
234/234 - 374s - loss: 0.7643 - accuracy: 0.7318 - mean_io_u: 0.2431 - val_loss: 0.7603 - val_accuracy: 0.7249 - val_mean_io_u: 0.2365 - 374s/epoch - 2s/step
Epoch 56/100

Epoch 56: val_loss improved from 0.76026 to 0.75629, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 55: Learning rate was 4.87406e-05.
234/234 - 374s - loss: 0.7610 - accuracy: 0.7365 - mean_io_u: 0.2452 - val_loss: 0.7563 - val_accuracy: 0.7296 - val_mean_io_u: 0.2380 - 374s/epoch - 2s/step
Epoch 57/100

Epoch 57: val_loss did not improve from 0.75629

Epoch 56: Learning rate was 4.77647e-05.
234/234 - 356s - loss: 0.7574 - accuracy: 0.7369 - mean_io_u: 0.2458 - val_loss: 0.7617 - val_accuracy: 0.7288 - val_mean_io_u: 0.2374 - 356s/epoch - 2s/step
Epoch 58/100

Epoch 58: val_loss did not improve from 0.75629

Epoch 57: Learning rate was 4.67866e-05.
234/234 - 353s - loss: 0.7583 - accuracy: 0.7375 - mean_io_u: 0.2460 - val_loss: 0.7610 - val_accuracy: 0.7291 - val_mean_io_u: 0.2385 - 353s/epoch - 2s/step
Epoch 59/100

Epoch 59: val_loss improved from 0.75629 to 0.75443, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 58: Learning rate was 4.58062e-05.
234/234 - 375s - loss: 0.7530 - accuracy: 0.7423 - mean_io_u: 0.2487 - val_loss: 0.7544 - val_accuracy: 0.7310 - val_mean_io_u: 0.2393 - 375s/epoch - 2s/step
Epoch 60/100

Epoch 60: val_loss did not improve from 0.75443

Epoch 59: Learning rate was 4.48235e-05.
234/234 - 356s - loss: 0.7536 - accuracy: 0.7422 - mean_io_u: 0.2487 - val_loss: 0.7585 - val_accuracy: 0.7269 - val_mean_io_u: 0.2372 - 356s/epoch - 2s/step
Epoch 61/100

Epoch 61: val_loss improved from 0.75443 to 0.75297, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 60: Learning rate was 4.38383e-06.
234/234 - 378s - loss: 0.7533 - accuracy: 0.7436 - mean_io_u: 0.2495 - val_loss: 0.7530 - val_accuracy: 0.7327 - val_mean_io_u: 0.2404 - 378s/epoch - 2s/step
Epoch 62/100

Epoch 62: val_loss did not improve from 0.75297

Epoch 61: Learning rate was 4.28507e-06.
234/234 - 352s - loss: 0.7528 - accuracy: 0.7433 - mean_io_u: 0.2502 - val_loss: 0.7595 - val_accuracy: 0.7274 - val_mean_io_u: 0.2350 - 352s/epoch - 2s/step
Epoch 63/100

Epoch 63: val_loss did not improve from 0.75297

Epoch 62: Learning rate was 4.18606e-06.
234/234 - 353s - loss: 0.7470 - accuracy: 0.7473 - mean_io_u: 0.2524 - val_loss: 0.7541 - val_accuracy: 0.7284 - val_mean_io_u: 0.2346 - 353s/epoch - 2s/step
Epoch 64/100

Epoch 64: val_loss did not improve from 0.75297

Epoch 63: Learning rate was 4.08678e-06.
234/234 - 352s - loss: 0.7450 - accuracy: 0.7481 - mean_io_u: 0.2523 - val_loss: 0.7626 - val_accuracy: 0.7210 - val_mean_io_u: 0.2292 - 352s/epoch - 2s/step
Epoch 65/100

Epoch 65: val_loss did not improve from 0.75297

Epoch 64: Learning rate was 3.98724e-06.
234/234 - 355s - loss: 0.7484 - accuracy: 0.7445 - mean_io_u: 0.2502 - val_loss: 0.7593 - val_accuracy: 0.7257 - val_mean_io_u: 0.2326 - 355s/epoch - 2s/step
Epoch 66/100

Epoch 66: val_loss did not improve from 0.75297

Epoch 65: Learning rate was 3.88742e-06.
234/234 - 354s - loss: 0.7443 - accuracy: 0.7490 - mean_io_u: 0.2528 - val_loss: 0.7570 - val_accuracy: 0.7253 - val_mean_io_u: 0.2324 - 354s/epoch - 2s/step
Epoch 67/100

Epoch 67: val_loss did not improve from 0.75297

Epoch 66: Learning rate was 3.78731e-06.
234/234 - 355s - loss: 0.7453 - accuracy: 0.7485 - mean_io_u: 0.2525 - val_loss: 0.7607 - val_accuracy: 0.7215 - val_mean_io_u: 0.2298 - 355s/epoch - 2s/step
Epoch 68/100

Epoch 68: val_loss did not improve from 0.75297

Epoch 67: Learning rate was 3.68691e-06.
234/234 - 351s - loss: 0.7502 - accuracy: 0.7449 - mean_io_u: 0.2504 - val_loss: 0.7575 - val_accuracy: 0.7252 - val_mean_io_u: 0.2323 - 351s/epoch - 1s/step
Epoch 69/100

Epoch 69: val_loss did not improve from 0.75297

Epoch 68: Learning rate was 3.5862e-06.
234/234 - 354s - loss: 0.7448 - accuracy: 0.7497 - mean_io_u: 0.2532 - val_loss: 0.7628 - val_accuracy: 0.7197 - val_mean_io_u: 0.2284 - 354s/epoch - 2s/step
Epoch 70/100

Epoch 70: val_loss did not improve from 0.75297

Epoch 69: Learning rate was 3.48518e-06.
234/234 - 351s - loss: 0.7484 - accuracy: 0.7465 - mean_io_u: 0.2512 - val_loss: 0.7588 - val_accuracy: 0.7225 - val_mean_io_u: 0.2303 - 351s/epoch - 2s/step
Epoch 71/100

Epoch 71: val_loss improved from 0.75297 to 0.75142, saving model to ./models/UNET_skip_100e_32b_Poly_Adam_milestones_warmup+DA_mid

Epoch 70: Learning rate was 3.38383e-06.
234/234 - 376s - loss: 0.7456 - accuracy: 0.7477 - mean_io_u: 0.2521 - val_loss: 0.7514 - val_accuracy: 0.7293 - val_mean_io_u: 0.2341 - 376s/epoch - 2s/step
Epoch 72/100

Epoch 72: val_loss did not improve from 0.75142

Epoch 71: Learning rate was 3.28215e-06.
234/234 - 354s - loss: 0.7471 - accuracy: 0.7475 - mean_io_u: 0.2519 - val_loss: 0.7607 - val_accuracy: 0.7240 - val_mean_io_u: 0.2302 - 354s/epoch - 2s/step
Epoch 73/100

Epoch 73: val_loss did not improve from 0.75142

Epoch 72: Learning rate was 3.18011e-06.
234/234 - 354s - loss: 0.7438 - accuracy: 0.7496 - mean_io_u: 0.2531 - val_loss: 0.7572 - val_accuracy: 0.7238 - val_mean_io_u: 0.2305 - 354s/epoch - 2s/step
Epoch 74/100

Epoch 74: val_loss did not improve from 0.75142

Epoch 73: Learning rate was 3.07771e-06.
234/234 - 354s - loss: 0.7437 - accuracy: 0.7490 - mean_io_u: 0.2527 - val_loss: 0.7652 - val_accuracy: 0.7208 - val_mean_io_u: 0.2291 - 354s/epoch - 2s/step
Epoch 75/100

Epoch 75: val_loss did not improve from 0.75142

Epoch 74: Learning rate was 2.97492e-06.
234/234 - 356s - loss: 0.7474 - accuracy: 0.7464 - mean_io_u: 0.2511 - val_loss: 0.7572 - val_accuracy: 0.7234 - val_mean_io_u: 0.2302 - 356s/epoch - 2s/step
Epoch 76/100
